# 模型评估与选择     
</br>
</br>
<b>2.1 经验误差与过拟合</b>   </br>        
分类错误的样本数占样本总数的比例称为“错误率”（error rate）; (1-错误率)* 100% 则称为精度（accuracy）.      
学习器的实际预测输出与样本的真实输出之间的差异称为“误差”（error）. 学习器在训练集上的误差称为训练误差（training error）或则经验误差（empirical error）. 在新样本上的误差称为泛化误差（generalization error）. 希望得到泛化误差小的学习器。      
当学习器把训练样本学得太好了，很可能已经把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这样就会导致泛化性能西安呢过，过拟合（overfitting）；相对的是，欠拟合（underfitting），指对训练样本的一般性质尚未学好。举个例子，训练样本是有锯齿的绿色叶子和椭圆的绿色叶子；过拟合会认为没有锯齿的树叶都不是树叶，欠拟合会认为数也是树叶因为有绿色的。         
只能缓解过拟合，无法避免。 P ！= NP.     
模型选择，理想情况，对候选模型的误差进行评估，然后选择泛化误差最小的那个模型。    
</br>
</br>
<b>2.2 评估方法  </b>     </br>   
通过实验测试来对学习器的泛化误差进行评估并进而做出选择。为此，需要一个测试集（testing ser）用来测试集上的测试误差（testing error）作为泛化误差的近似。    
测试样本尽量不在训练集中出现，为在训练过程中使用过。      
如何对数据集D 进行适当的处理，从中产生出训练集S 和测试集T .    </br>     
2.2.1 留出法    </br>
hold out, 直接将数据集D 划分为两个互斥的集合，其中一个作为训练集S，另外一个作为测试集T. 在S 上训练处模型后，用T 来评估其测试误差，作为对泛化误差的估计。    
训练/测试集的划分要尽可能保持数据分布的一致性，划分过程中应避免引入额外的偏差。    
从采样的角度，保留类别比例的采样方式成为分层采样（stratified sampling），样本类别比例差别要保持一致或很小。    
优化：若干次随机划分，重复进行实验评估后取平均值作为留出法的评估结果。可以得到多次结果的平均值和标准差。        
缺点：若训练集S 包含大多数样本，训练出的模型可能更接近于用D 训练出的模型，但由于T 比较小，评估结果可能不够稳定准确，结果的方差较大；若测试集T 包含多一些样本，则训练集S 和D 差别更大，被评估的模型与用D 训练出的模型相比可以有较大差别，结果偏差较大，降低了评估结果的保真性（fidelity）.    
常见做法：没有完美的解决方案，将大约2/3~4/5的样本用于训练，剩余样本用于测试，测试集一般而言。     </br>   
2.2.2 交叉验证法    </br>   
cross validation, 将数据集D 划分为k 个大小相似的互斥子集，每个自己Di 都尽可能保持数据分布的一致性；每次用k-1 个子集的并集作为训练集，剩下的那个子集作为测试集；最终返回的是这k 个测试结果的均值。    
交叉验证法评估结果的稳定性和保真性在很大程度上取决于k 的取值；所以此方法也叫k 折交叉验证（k-fold cross validation）. k 的取值最常用为10， 其次为5,20.    
将数据集D 划分为k 个子集同样存在多种划分方式，为减小因样本划分不同而引入的差别，k 折交叉验证通常要随机使用不同的划分重复p 次，最终的评估结果是这p 次k 折交叉验证结果的均值。举个例子，10次10折交叉验证法与100次留出法都是进行了100次训练/测试。    
有一个特殊性，假设数据集D 中包含m 个样本，若k = m，则得到了留一法（Leave-One-Out, LOO）. 该方法只和初始数据集相比少了一个样本，结果会很相似，但是时间成本太高，“面有免费的午餐”定理对其也有效。      </br>   
2.2.3 自助法    </br>   
bootstrapping；减少训练样本规模不同造成的影响，同时还能比较高效地进行试验估计；以自助采样法为基础，给定包含m 个样本的数据集D，对它进行采样产生数据集D'，每次随机从D 中挑选一个样本，将其拷贝到D'，然后再放回D 中，这样下次采样时仍有可能被采到；这个过程重复执行m 次后，就得到了包含m 个样本的数据集D'. 显然，D 中有一部分样本会在D' 中多次出现，而另一部分样本不出现。    
一个简单的估计，样本在m 次采样中始终不被采到的概率是（1-1/m）^m. 取极限得到1/e 约是0.368.   
于是，D' 可当做训练集，D/D' 用作测试集；这样，实际评估的模型和期望评估的模型都使用m 个训练样本，仍然有数据总量约1/3，没在训练集中出现的样本用于测试，这样的测试结果也被称为“包外估计”（out-of-bag estimate）.     
自助法在数据集较小，难以有效划分训练/测试集时候很有效，而且能从初始数据集中产生多个不同的训练集，对集成学习等方法有很大好处；然而，自助法产生的数据集改变了初始数据集的分布，会引入估计偏差。因此，在初始数据量足够的时候，留出法和交叉验证法更常用一些。      </br>  
2.2.4 调参与最终模型     </br> 
参数(parameter)配置不同，学得模型的性能往往有显著差别。     
参数调节，简称调参（parameter tuning），对算法参数进行设定。学习算法的很多参数是在实数范围内取值的。      
现实中的做法，对每一个参数选定一个范围和变化步长，例如在[0,0.2] 范围内以0.05 为步长，则实际要评估的候选参数值有5个，最终从这5个候选值中产生选定值；这是在计算开销和性能估计之间进行折中的结果。      
调参很困难，假设算法有3 个参数，每个参数仅考虑5 个候选值，这样对每一组训练/测试集就有5^3 = 125 个模型需要考察。    
学得模型在实际使用中遇到的数据称为测试数据；模型评估与选择中用于评估测试的数据集常称为验证集（validation set）.    

</br>
</br>
<b>2.3 性能度量</b>   </br>   














