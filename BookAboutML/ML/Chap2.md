# 模型评估与选择     
</br>
</br>
<b>2.1 经验误差与过拟合</b>   </br>        
分类错误的样本数占样本总数的比例称为“错误率”（error rate）; (1-错误率)* 100% 则称为精度（accuracy）.      
学习器的实际预测输出与样本的真实输出之间的差异称为“误差”（error）. 学习器在训练集上的误差称为训练误差（training error）或则经验误差（empirical error）. 在新样本上的误差称为泛化误差（generalization error）. 希望得到泛化误差小的学习器。      
当学习器把训练样本学得太好了，很可能已经把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这样就会导致泛化性能西安呢过，过拟合（overfitting）；相对的是，欠拟合（underfitting），指对训练样本的一般性质尚未学好。举个例子，训练样本是有锯齿的绿色叶子和椭圆的绿色叶子；过拟合会认为没有锯齿的树叶都不是树叶，欠拟合会认为数也是树叶因为有绿色的。         
只能缓解过拟合，无法避免。 P ！= NP.     
模型选择，理想情况，对候选模型的误差进行评估，然后选择泛化误差最小的那个模型。    
</br>
</br>
<b>2.2 评估方法  </b>     </br>   
通过实验测试来对学习器的泛化误差进行评估并进而做出选择。为此，需要一个测试集（testing ser）用来测试集上的测试误差（testing error）作为泛化误差的近似。    
测试样本尽量不在训练集中出现，为在训练过程中使用过。      
如何对数据集D 进行适当的处理，从中产生出训练集S 和测试集T .    </br>     
2.2.1 留出法    </br>
hold out, 直接将数据集D 划分为两个互斥的集合，其中一个作为训练集S，另外一个作为测试集T. 在S 上训练处模型后，用T 来评估其测试误差，作为对泛化误差的估计。    
训练/测试集的划分要尽可能保持数据分布的一致性，划分过程中应避免引入额外的偏差。    
从采样的角度，保留类别比例的采样方式成为分层采样（stratified sampling），样本类别比例差别要保持一致或很小。    
优化：若干次随机划分，重复进行实验评估后取平均值作为留出法的评估结果。可以得到多次结果的平均值和标准差。        
缺点：若训练集S 包含大多数样本，训练出的模型可能更接近于用D 训练出的模型，但由于T 比较小，评估结果可能不够稳定准确，结果的方差较大；若测试集T 包含多一些样本，则训练集S 和D 差别更大，被评估的模型与用D 训练出的模型相比可以有较大差别，结果偏差较大，降低了评估结果的保真性（fidelity）.    
常见做法：没有完美的解决方案，将大约2/3~4/5的样本用于训练，剩余样本用于测试，测试集一般而言。     </br>   
2.2.2 交叉验证法    </br>   
cross validation, 将数据集D 划分为k 个大小相似的互斥子集，每个自己Di 都尽可能保持数据分布的一致性；每次用k-1 个子集的并集作为训练集，剩下的那个子集作为测试集；最终返回的是这k 个测试结果的均值。    
交叉验证法评估结果的稳定性和保真性在很大程度上取决于k 的取值；所以此方法也叫k 折交叉验证（k-fold cross validation）. k 的取值最常用为10， 其次为5,20.    
将数据集D 划分为k 个子集同样存在多种划分方式，为减小因样本划分不同而引入的差别，k 折交叉验证通常要随机使用不同的划分重复p 次，最终的评估结果是这p 次k 折交叉验证结果的均值。举个例子，10次10折交叉验证法与100次留出法都是进行了100次训练/测试。    
有一个特殊性，假设数据集D 中包含m 个样本，若k = m，则得到了留一法（Leave-One-Out, LOO）. 该方法只和初始数据集相比少了一个样本，结果会很相似，但是时间成本太高，“面有免费的午餐”定理对其也有效。      </br>   
2.2.3 自助法    </br>   
bootstrapping；减少训练样本规模不同造成的影响，同时还能比较高效地进行试验估计；以自助采样法为基础，给定包含m 个样本的数据集D，对它进行采样产生数据集D'，每次随机从D 中挑选一个样本，将其拷贝到D'，然后再放回D 中，这样下次采样时仍有可能被采到；这个过程重复执行m 次后，就得到了包含m 个样本的数据集D'. 显然，D 中有一部分样本会在D' 中多次出现，而另一部分样本不出现。    
一个简单的估计，样本在m 次采样中始终不被采到的概率是（1-1/m）^m. 取极限得到1/e 约是0.368.   
于是，D' 可当做训练集，D/D' 用作测试集；这样，实际评估的模型和期望评估的模型都使用m 个训练样本，仍然有数据总量约1/3，没在训练集中出现的样本用于测试，这样的测试结果也被称为“包外估计”（out-of-bag estimate）.     
自助法在数据集较小，难以有效划分训练/测试集时候很有效，而且能从初始数据集中产生多个不同的训练集，对集成学习等方法有很大好处；然而，自助法产生的数据集改变了初始数据集的分布，会引入估计偏差。因此，在初始数据量足够的时候，留出法和交叉验证法更常用一些。      </br>  
2.2.4 调参与最终模型     </br> 
参数(parameter)配置不同，学得模型的性能往往有显著差别。     
参数调节，简称调参（parameter tuning），对算法参数进行设定。学习算法的很多参数是在实数范围内取值的。      
现实中的做法，对每一个参数选定一个范围和变化步长，例如在[0,0.2] 范围内以0.05 为步长，则实际要评估的候选参数值有5个，最终从这5个候选值中产生选定值；这是在计算开销和性能估计之间进行折中的结果。      
调参很困难，假设算法有3 个参数，每个参数仅考虑5 个候选值，这样对每一组训练/测试集就有5^3 = 125 个模型需要考察。    
学得模型在实际使用中遇到的数据称为测试数据；模型评估与选择中用于评估测试的数据集常称为验证集（validation set）.    

</br>
</br>
<b>2.3 性能度量</b>   </br>   
有衡量模型泛化能力的评价标准。    
性能度量反映了任务需求。对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果；意味着模型的“好坏”是相对的，取决于算法，数据，任务需求。   
评估学习器f 的性能，就要把学习器预测结果f(x) 与真实标记y 进行比较。    
回归任务最常用的性能度量是“均方误差”（mean squared error）.    </br>   
2.3.1 错误率与精读    </br>    
分类任务中最常用的两种性能度量，错误率(分类错误的样本数占样本总数的比例)和精度(分类正确的样本数占样本总数的比例)，适用于二分类任务和多分类任务。   </br>   
2.3.2 查准率，查全率与F1    </br>      
信息检索中，“检索出的信息中有多少比例是用户感兴趣的”，“用户感兴趣的信息中有多少被检索出来了”，查准率（precision）亦称“准确率”，查全率（recall）亦称召回率。    
对于二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例(true positive)，假正例(false positive)，真反例(true negative)，假反例(false negative). 显然有TP+FP+TN+FN=样例总数。   
查准率P = TP/(TP+FP)；查全率R = TP/(TP+FN).     
查准率和查全率是一对矛盾的度量。一般来说，查准率高时，查全率往往偏低；查全率高时，查准率往往偏低。     
对样例进行排序，排在前面的是学习器认为“最可能”是正例的样本，排在最后的则是学习其认为“最不可能”是正例的样本。
以查准率为纵轴，查全率为横轴做图，就得到了查准率-查全率曲线，简称“P-R曲线”（现实中是非单调，不平滑的，在很多局部上有上下波动），现实该曲线的图称为“P-R图”。     
哪个学习器更好，看曲线下的面积；综合考虑precision, recall 的性能度量，一个是平衡点 break-event point, 是 p = r 时候的取值，平衡点越大说明学习器越好。另外更好的是F1 度量，其一般形式Fβ 可以表达出对p/c 的不同偏好，β大于1，查全率影响更大，β小于1，春准率影响更大。     
</br>   
2.3.3 ROC 与 AUC    </br>    
threshold - cut point. 对测试样本进行排序，“最可能”是正例的排在最前面，“最不可能”是正例的排在最后面。      
如果更侧重查准率P，则选择排序中靠前的位置进行截断；如果更侧重查全率R，则选择靠后的位置进行截断。     
ROC，Receiver Operating Characteristic/受试者工作特征 曲线。根据学习器的预测结果对样例进行排序，按此顺序逐个把样本作为正例进行预测，每次计算出两个重要量的值，分别以它们为横，纵坐标作图，得到“ROC曲线”。横轴是“假正例率”（False Positive Rate）FPR = FP/(TN+FP)，纵轴是“真正例率”（True Positive Rate）TPR = TP/(TP+FN).      
比较两个学习器的ROC，如果两个曲线发生交叉，难以一般性地断言两者孰好孰坏；如果一定要进行比较，较为合理的判断依据是比较ROC 曲线下的面积，即AUC（Area Under ROC Curve）, 可通过对ROC 曲线下各部分的面积求和而得。AUC 考虑的是样本预测的排序质量，与排序误差有紧密的联系。考虑每一对正，反例，若正例的预测值小于反例，则记一个“罚分”；若相等，则记0.5个“罚分”。排序“损失”（loss），loss (rank) 对应的是ROC 曲线之上的面积，AUC = 1 - loss (rank).    
</br>   
2.3.4 代价敏感错误率与代价曲线    </br>    
为权衡不同类型错误所造成的不同损失，可以为错误赋予“非均等代价”（unequal cost）.     
可根据任务的领域知识设定一个“代价矩阵”（matrix cost）. 
在非均等代价下，我们所希望的是最小化“总体代价”（total cost）. 代价敏感 cost-sensitive.    
在非均等代价下，ROC 曲线不能直接反映出学习器的期望总体代价，而代价曲线（cost curve）可以达到目的。      

</br>
</br>
<b>2.4 比较检验</b>   </br>  


















