# 模型评估与选择     
</br>
</br>
<b>2.1 经验误差与过拟合</b>   </br>        
分类错误的样本数占样本总数的比例称为“错误率”（error rate）; (1-错误率)* 100% 则称为精度（accuracy）.      
学习器的实际预测输出与样本的真实输出之间的差异称为“误差”（error）. 学习器在训练集上的误差称为训练误差（training error）或则经验误差（empirical error）. 在新样本上的误差称为泛化误差（generalization error）. 希望得到泛化误差小的学习器。      
当学习器把训练样本学得太好了，很可能已经把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这样就会导致泛化性能西安呢过，过拟合（overfitting）；相对的是，欠拟合（underfitting），指对训练样本的一般性质尚未学好。举个例子，训练样本是有锯齿的绿色叶子和椭圆的绿色叶子；过拟合会认为没有锯齿的树叶都不是树叶，欠拟合会认为数也是树叶因为有绿色的。         
只能缓解过拟合，无法避免。 P ！= NP.     
模型选择，理想情况，对候选模型的误差进行评估，然后选择泛化误差最小的那个模型。    
</br>
</br>
<b>2.2 评估方法  </b>     </br>   
通过实验测试来对学习器的泛化误差进行评估并进而做出选择。为此，需要一个测试集（testing ser）用来测试集上的测试误差（testing error）作为泛化误差的近似。    
测试样本尽量不在训练集中出现，为在训练过程中使用过。      
如何对数据集D 进行适当的处理，从中产生出训练集S 和测试集T .    </br>     
2.2.1 留出法    </br>
hold out, 直接将数据集D 划分为两个互斥的集合，其中一个作为训练集S，另外一个作为测试集T. 在S 上训练处模型后，用T 来评估其测试误差，作为对泛化误差的估计。    
训练/测试集的划分要尽可能保持数据分布的一致性，划分过程中应避免引入额外的偏差。    
从采样的角度，保留类别比例的采样方式成为分层采样（stratified sampling），样本类别比例差别要保持一致或很小。    
优化：若干次随机划分，重复进行实验评估后取平均值作为留出法的评估结果。可以得到多次结果的平均值和标准差。        
缺点：若训练集S 包含大多数样本，训练出的模型可能更接近于用D 训练出的模型，但由于T 比较小，评估结果可能不够稳定准确，结果的方差较大；若测试集T 包含多一些样本，则训练集S 和D 差别更大，被评估的模型与用D 训练出的模型相比可以有较大差别，结果偏差较大，降低了评估结果的保真性（fidelity）.    
常见做法：没有完美的解决方案，将大约2/3~4/5的样本用于训练，剩余样本用于测试，测试集一般而言。     </br>   
2.2.2 交叉验证法    </br>   
cross validation, 将数据集D 划分为k 个大小相似的互斥子集，每个自己Di 都尽可能保持数据分布的一致性；每次用k-1 个子集的并集作为训练集，剩下的那个子集作为测试集；最终返回的是这k 个测试结果的均值。    
交叉验证法评估结果的稳定性和保真性在很大程度上取决于k 的取值；所以此方法也叫k 折交叉验证（k-fold cross validation）. k 的取值最常用为10， 其次为5,20.    
将数据集D 划分为k 个子集同样存在多种划分方式，为减小因样本划分不同而引入的差别，k 折交叉验证通常要随机使用不同的划分重复p 次，最终的评估结果是这p 次k 折交叉验证结果的均值。举个例子，10次10折交叉验证法与100次留出法都是进行了100次训练/测试。    
有一个特殊性，假设数据集D 中包含m 个样本，若k = m，则得到了留一法（Leave-One-Out, LOO）. 该方法只和初始数据集相比少了一个样本，结果会很相似，但是时间成本太高，“面有免费的午餐”定理对其也有效。      </br>   
2.2.3 自助法    </br>   











